[ [Español](README_es.md) ] [ [简体中文](README_zhcn.md) ] [ [繁體中文](README_zhtw.md) ] [ [日本語](README_ja.md) ] [ [한국어](README_ko.md) ] 

# AI Power

AI technology is advancing at lightning speed, with new algorithms and AI libraries emerging and evolving constantly. To empower more people to master the latest AI innovations and actively participate in open-source projects, I created AI Power. Join us in exploring the cutting-edge of AI technology and contribute to shaping the future!

## Article Categories

### Code-Driven Algorithm Learning

| Title                 | Description           | Keywords              |
|-----------------------|-----------------------|-----------------------|
| [Understanding Transformer Attentions](deep_learning/transformer/transformer_attentions.md)                   | An in-depth explanation of the attention mechanism in transformers, covering self-attention, multi-head attention, and their implementation in modern NLP models. | Transformers, Self-Attention, MHA |
| [The Vanilla Transformer Explained](deep_learning/transformer/vanilla_transformer_explained_enus.md)       | A comprehensive guide to the vanilla transformer model, detailing its architecture, components, and the forward pass process for sequence-to-sequence tasks. | vanilla Transformer, Architecture, Sequence-to-Sequence |
| [Inside CLIP](deep_learning/transformer/model_multimodal/openai_clip_explained.md)               | An in-depth explanation of the CLIP model, covering its architecture, training process, and applications in linking images and text. | CLIP, Architecture |
| [Deep Dive into LLaVA](deep_learning/transformer/model_multimodal/llava_implementation_explained.md)           | A comprehensive guide to the implementation of the LLaVA model, exploring its architecture, components, and how it enhances language understanding tasks. | LLaVA, Architecture, MultiModal |
| [Deep Dive into Vision Transformer](deep_learning/transformer/model_vision/huggingface_vit_explained.md)                | An in-depth explanation of the Vision Transformer (ViT) model, detailing its architecture, key components, and application in computer vision tasks. | Vision Transformer, ViT, Architecture |
| [AutoEncoder Explained](deep_learning/diffusion/autoencoder_explained.md)                    | An in-depth explanation of autoencoders, their architecture, types, and applications in data compression and feature learning. | AutoEncoder, VAE, Architecture |

### Huggingface API

| Title                 | Description           | Keywords              |
|-----------------------|-----------------------|-----------------------|
| [Huggingface SBERT API - Part 1](deep_learning/transformer/huggingface_sbert_api_part1.md) | This article introduces the Huggingface Sentence-BERT (SBERT) API, explaining its purpose, applications, and how to use it for embedding sentences and computing similarities. | SBERT, Sentence Transformer, Embeddings         |
| [Huggingface SBERT API - Part 2](deep_learning/transformer/huggingface_sbert_api_part2.md) | This continuation of the SBERT API guide covers advanced usage, fine-tuning models, and integrating SBERT with various applications. | SBERT, Sentence Transformer, Embeddings |
| [Huggingface Transformer Auto Class API - Part 1](deep_learning/transformer/huggingface_transformer_auto_class_api_part1.md) | This article provides an overview of the Huggingface Transformer Auto Class API, detailing its features, setup, and basic usage for different NLP tasks. | Transformer, Auto Class, NLP, API             |
| [Huggingface Transformer Auto Class API - Part 2](deep_learning/transformer/huggingface_transformer_auto_class_api_part2.md) | A deeper dive into the Transformer Auto Class API, exploring custom configurations, model optimization, and use cases. | Transformer, Auto Class, NLP, API |
| [Huggingface Transformer Pipeline API](deep_learning/transformer/huggingface_transformer_pipeline_api.md) | This guide explains the Huggingface Transformer Pipeline API, showcasing its ease of use for various NLP tasks like text classification, named entity recognition, and text generation. | Transformer Pipeline |
| [Huggingface CLIP API](deep_learning/transformer/model_multimodal/huggingface_clip_api.md)                     | This article introduces the Huggingface CLIP (Contrastive Language-Image Pre-training) API, explaining its purpose, applications, and how to use it for image and text embeddings. | Huggingface, CLIP, API |
| [Huggingface LLaVA Next API](deep_learning/transformer/model_multimodal/huggingface_llava_next_api.md)               | This guide details the Huggingface LLaVA Next API, outlining its features, setup, and usage for advanced language understanding tasks. | LLaVA, MultiModal, API |
| [Huggingface Vision Transformer (ViT) API](deep_learning/transformer/model_vision/huggingface_vit_api.md)       | This article provides an overview of the Huggingface Vision Transformer (ViT) API, explaining its usage for image classification and other vision tasks. | ViT, Vision Transformer, Image Classification, API |
| [Huggingface Diffusers API](deep_learning/diffusion/huggingface_diffusers_api.md)                | This article provides an overview of the Huggingface Diffusers API, explaining its functionality and usage for generating images from text descriptions. | Diffusers API, Image Generation, Text-to-Image, Image-to-Image |
| [Huggingface Diffusers Chained Pipeline](deep_learning/diffusion/huggingface_diffusers_chained_pipeline.md)   | This guide explains how to create chained pipelines using the Huggingface Diffusers API, showcasing how to combine multiple models for complex tasks. | Diffusers API, Chained Pipeline |
| [Huggingface Diffusers Pipeline API](deep_learning/diffusion/huggingface_diffusers_pipeline_api.md)       | An in-depth look at the Huggingface Diffusers Pipeline API, detailing its features, setup, and applications for image and text generation. | Diffusers Pipeline API |

### Tensor Operation

| Title                 | Description           | Keywords              |
|-----------------------|-----------------------|-----------------------|
| [Einops Einsum](deep_learning/coding/einops_einsum.md)                            | An introduction to the einsum function in the einops library, explaining its syntax, usage, and applications in tensor operations. | einops, einsum, tensor operations |
| [Einops Rearrange](deep_learning/coding/einops_rearrange.md)                         | This guide details the rearrange function in the einops library, showcasing how to efficiently manipulate and transform tensor shapes. | einops, einsum, tensor operations |

### Dataset

| Title                 | Description           | Keywords              |
|-----------------------|-----------------------|-----------------------|
| [Huggingface Datasets Loading](deep_learning/dataset/huggingface_datasets_loading.md)           | This article covers how to load and preprocess datasets using the Huggingface Datasets library, including handling various data formats and sources. | Huggingface, Datasets |
| [Huggingface Datasets Main Classes](deep_learning/dataset/huggingface_datasets_main_classes.md) | A comprehensive guide to the main classes of the Huggingface Datasets library, explaining their functionalities and use cases. | Huggingface, Datasets, Main Classes |
| [Alpaca Self-Instruct Guide](deep_learning/dataset/alpaca_self_instruct_guide.md)           | This guide provides a comprehensive overview of the Self-Instruct process using Alpaca, including step-by-step instructions and examples. | Alpaca, Self-Instruct |
| [Generating Datasets with Unstructured.io and GPT4](deep_learning/dataset/generate_dataset_with_unstructureio_gpt4.md) | This article demonstrates how to use Unstructured.io and GPT-4 to process PDF files and generate datasets by extracting and organizing content. | Unstructured.io, GPT-4 |
| [Generating Datasets with Table Transformer and GPT4](deep_learning/dataset/generate_dataset_with_table_transformer.md) | This article explains how to use Table Transformer and GPT-4 to generate datasets from PDF files by detecting and extracting table structures. | Table Transformer, GPT-4 |


### Model Training

| Title                 | Description           | Keywords              |
|-----------------------|-----------------------|-----------------------|
| [SFT: Model Architecture Tweaks](deep_learning/training/sft_model_arch_tweaks.md) | This article discusses various tweaks and adjustments to model architecture for optimizing performance in supervised fine-tuning. | Model Architecture, SFT |
| [SFT: Training Strategy](deep_learning/training/sft_train_strategy.md)       | This article provides insights into effective training strategies for supervised fine-tuning, including tips and best practices. | Training Strategy, SFT |
| [SFT: Data Handling](deep_learning/training/sft_data_handling.md)            | This article explains the techniques for handling and preparing data for supervised fine-tuning tasks. | Data Handling, SFT   |
| [SFT: Loss Function](deep_learning/training/sft_loss_function.md)            | This article explores different loss functions used in supervised fine-tuning and their impact on model performance. | Loss Function, SFT   |
| [Huggingface Transformer Trainer API](deep_learning/training/huggingface_transformer_trainer_finetune.md) | This guide explores how to fine-tune transformer models using the Huggingface Trainer API, covering setup, training, and evaluation processes. | Huggingface, Transformer, Trainer API, SFT |
| [Huggingface Evaluate API](deep_learning/training/huggingface_evaluate_api.md) | This article introduces the Huggingface Evaluate API, detailing its purpose, setup, and usage for evaluating machine learning models. | Huggingface, Evaluate API, Metric |


## Our Goals

The goals of AI Power Source are as follows:

- **Effectively Understand AI Algorithms**: Deepen understanding of various AI algorithms by reading code.
- **Quickly Learn AI Libraries**: Accelerate mastery of various AI libraries using numerous example programs.
- **Analyze Code**: Promote learning by analyzing the code of various AI frameworks and applications.
- **Learn Model Training Techniques**: Quickly master the skills required for AI model training through abundant example programs and experience sharing.
- **MLOps Process Design**: Learn and practice MLOps process design to improve the efficiency and reliability of model deployment and management.
- **System Architecture Design**: Learn the system architecture design of AI applications through case studies, including software and cloud architecture design.

## How to Contribute

If you wish to assist with the following tasks, we welcome you to join us:

- **Assist in Translating Existing Articles**: Help translate and improve existing articles and educational materials.
- **Contribute New Articles**: Contribute at least one new article each month, sharing your AI knowledge and experience.

## Join Us

We welcome all individuals interested in AI technology to join our community, regardless of your experience level. Every contribution you make will play a significant role in promoting the popularization and development of AI technology.

## Contact Us

If you have any questions or suggestions, please contact us via [GitHub Issues](https://github.com/ksmooi/ai_power/issues).

Thank you for your participation and support!